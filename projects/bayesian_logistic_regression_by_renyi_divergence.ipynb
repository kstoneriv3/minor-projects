{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $n=1,...,N$,\n",
    "\\begin{align}\n",
    "\\beta &\\sim N(0,I)\\\\ \n",
    "Y_n &\\sim \\text{Bernoulli}\\left(\\frac{1}{1+\\exp(-\\beta^T x_n)}\\right)\n",
    "\\end{align}.\n",
    "This models the binary outcome $y_n$'s using the covariate $x_n$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import breast cancer dataset for binary classification\n",
    "x,y = load_breast_cancer(return_X_y=True)\n",
    "dim_x = x.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_float_type = tf.float64\n",
    "np_float_type = np.float64\n",
    "as_tf_float = lambda x: tf.cast(x, tf_float_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_logsumexp(ary, axis=1, keepdims=False):\n",
    "    return tf.math.reduce_logsumexp(ary, axis=axis, keepdims=keepdims)\n",
    "\n",
    "def tf_logmeanexp(ary, axis=1, keepdims=False):\n",
    "    return tf.math.reduce_logsumexp(ary, axis=axis, keepdims=keepdims) \\\n",
    "        - tf.math.log(as_tf_float(ary.shape[axis]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Non-Bayesian) Logistic Regression on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LogisticRegression(max_iter=10000).fit(x,y)\n",
    "score = m.score(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.9578207381370826\n"
     ]
    }
   ],
   "source": [
    "print('R2 score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variatioal Bayes by KL Divergence\n",
    "Our estimator of $q(\\beta)$ (⇔`loc_beta` and `scale_beta`) are obtained as:\n",
    "$$\\hat q_{\\text{KL}} = \\arg\\min \\mathrm{KL}[q(\\beta)||p(\\beta|y_{1:N};x_{1:N})],$$\n",
    "where KL divergence is defined as \n",
    "$$\\mathrm{KL}[q(x)||p(x)] = \\mathrm{E}_{X\\sim q(x)}\\left[\\log\\frac{q(X)}{p(X)}\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_ELBO(loc_beta, scale_beta, beta):\n",
    "    \"\"\"\n",
    "    Compute ELBO for n_MC samples of beta, where the shape of beta is [n_MC, dim_x].\n",
    "    \n",
    "    Arguments:\n",
    "    loc_beta: 1-d array of size [dim_x]\n",
    "    scale_beta: 1-d array of size [dim_x]\n",
    "    beta: 2-d array of size [n_MC, dim_x] representing the n_MC samples of beta taken from q_beta\n",
    "    \n",
    "    Returns:\n",
    "    elbo: n_MC MC-samples of elbo\n",
    "    \"\"\"\n",
    "    p_beta = tfp.distributions.Normal(loc=np.zeros([dim_x]), scale=np.ones([dim_x]))\n",
    "    q_beta = tfp.distributions.Normal(loc=loc_beta, scale=scale_beta)\n",
    "    z = beta @ x.T# of shape [n_MC, n_train]\n",
    "    p_y = tfp.distributions.Bernoulli(logits=z)\n",
    "    y_ = y.reshape([1, -1])\n",
    "    elbo = tf.reduce_sum( p_y.log_prob(y_), axis=1)\\\n",
    "            + tf.reduce_sum( p_beta.log_prob(beta), axis=1)\\\n",
    "            - tf.reduce_sum( q_beta.log_prob(beta), axis=1)\n",
    "    return elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(loc_beta, scale_beta, n_MC=1):\n",
    "    q_beta = tfp.distributions.Normal(loc=loc_beta, scale=scale_beta)\n",
    "    beta = q_beta.sample(n_MC)# of shape [n_MC, dim_x]\n",
    "    elbo = tf.reduce_mean( pointwise_ELBO(loc_beta, scale_beta, beta) )\n",
    "    return elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-19761.456766640746, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "loc = tf.Variable(np.zeros([dim_x]))\n",
    "invSPscale = tf.Variable(np.ones([dim_x])) # defined as inv_softplus(scale) to keep the positive constraint of scale\n",
    "    \n",
    "for t in range(4000):\n",
    "    rho_t = 5e-6/(10+t)**0.7\n",
    "    \n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch([loc, invSPscale])\n",
    "        scale = tf.math.softplus(invSPscale) \n",
    "        score = ELBO(loc, scale, 100)\n",
    "    dloc, dinvSPscale = g.gradient(score, [loc, invSPscale])\n",
    "\n",
    "    loc = loc + rho_t*dloc\n",
    "    invSPscale = invSPscale + rho_t*dinvSPscale\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score by plug-in prediction: 0.9068541300527241\n"
     ]
    }
   ],
   "source": [
    "score = np.mean((x @ loc.numpy()>0)==y) # accuracy of binary prediction\n",
    "print('R2 score by plug-in prediction: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Bayes by Renyi (or $\\chi$) Divergence\n",
    "Our estimator of $q(\\beta)$ (⇔`loc_beta` and `scale_beta`) are obtained as:\n",
    "$$\\hat q_{\\text{KL}} = \\arg\\min \\mathrm{D}_{1/2}[q(\\beta)||p(\\beta|y_{1:N};x_{1:N})],$$\n",
    "where Renyi (or $\\chi$) divergence is defined as \n",
    "$$\\mathrm{D}_\\gamma[q(x)||p(x)] = \\frac{1}{\\gamma}\\log\\mathrm{E}_{X\\sim q(x)}\\left(\\frac{p(X)}{q(X)}\\right)^{\\gamma}.$$\n",
    "When $\\gamma=1-\\alpha$ for $\\alpha>0$, above quantity becomes Renyi's $\\alpha$-divergence. When $\\gamma>1$, $-\\mathrm{D}_\\gamma[q(x)||p(x)]$ becomes $\\chi$-divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_Renyi_Chi(loc_beta, scale_beta, beta, gamma):\n",
    "    \"\"\"\n",
    "    Compute ELBO for n_MC samples of beta, where the shape of beta is [n_MC, dim_x].\n",
    "    \n",
    "    Arguments:\n",
    "    loc_beta: 1-d array of size [dim_x]\n",
    "    scale_beta: 1-d array of size [dim_x]\n",
    "    beta: 3-d array of size [n_MC_out, n_MC_in, dim_x] representing the n_MC samples of beta taken from q_beta\n",
    "    \n",
    "    Returns:\n",
    "    elbo: n_MC MC-samples of elbo\n",
    "    \"\"\"\n",
    "\n",
    "    p_beta = tfp.distributions.Normal(loc=np.zeros([dim_x]), scale=np.ones([dim_x]))\n",
    "    q_beta = tfp.distributions.Normal(loc=loc_beta, scale=scale_beta)\n",
    "    z = beta @ x.T# of shape [n_MC_out, n_MC_in, n_train]\n",
    "    p_y = tfp.distributions.Bernoulli(logits=z)\n",
    "    log_prob_ratio = tf.reduce_sum( p_y.log_prob(y), axis=2)\\\n",
    "                    + tf.reduce_sum( p_beta.log_prob(beta), axis=2)\\\n",
    "                    - tf.reduce_sum( q_beta.log_prob(beta), axis=2)# of shape [n_MC_out, n_MC_in]\n",
    "    score = 1./gamma * tf_logmeanexp(gamma * log_prob_ratio, axis=1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Renyi_Chi(loc_beta, scale_beta, gamma, n_MC_out=1, n_MC_in=1):\n",
    "    q_beta = tfp.distributions.Normal(loc=loc_beta, scale=scale_beta)\n",
    "    beta = q_beta.sample([n_MC_out, n_MC_in])# of shape [n_MC_out, n_MC_in, dim_x]\n",
    "    score = tf.reduce_mean( pointwise_Renyi_Chi(loc_beta, scale_beta, beta, gamma) )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-1431.275781856723, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "loc = tf.Variable(np.zeros([dim_x]))\n",
    "invSPscale = tf.Variable(np.ones([dim_x])) # defined as inv_softplus(scale) to keep the positive constraint of scale\n",
    "    \n",
    "for t in range(4000):\n",
    "    rho_t = 1e-4/(10+t)**0.7\n",
    "    \n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch([loc, invSPscale])\n",
    "        scale = tf.math.softplus(invSPscale) \n",
    "        score = Renyi_Chi(loc, scale, gamma=1./2, n_MC_out=100, n_MC_in=20)\n",
    "    dloc, dinvSPscale = g.gradient(score, [loc, invSPscale])\n",
    "\n",
    "    loc = loc + rho_t*dloc\n",
    "    invSPscale = invSPscale + rho_t*dinvSPscale\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score by plug-in prediction: 0.9103690685413005\n"
     ]
    }
   ],
   "source": [
    "score = np.mean((x @ loc.numpy()>0)==y) # accuracy of binary prediction\n",
    "print('R2 score by plug-in prediction: {}'.format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
